{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2310932,"sourceType":"datasetVersion","datasetId":1394179},{"sourceId":11251419,"sourceType":"datasetVersion","datasetId":7030981},{"sourceId":11251558,"sourceType":"datasetVersion","datasetId":7031089},{"sourceId":11252787,"sourceType":"datasetVersion","datasetId":7031988}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"For yolov3 you can prefer: 1- https://pjreddie.com/media/files/papers/YOLOv3.pdf and 2- https://pjreddie.com/darknet/yolo/","metadata":{}},{"cell_type":"code","source":"# Que 1\n\n!git clone https://github.com/pjreddie/darknet\n%cd darknet\n!make","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://pjreddie.com/media/files/yolov3.weights","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!./darknet detect cfg/yolov3.cfg yolov3.weights /kaggle/input/cat-and-dogs/dataset/test_set/cats/cat.4004.jpg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Implementing the above with open-cv","metadata":{}},{"cell_type":"code","source":"!pip install opencv-python numpy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://pjreddie.com/media/files/yolov3.weights \n!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg \n!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# Load YOLOv3 model\nnet = cv2.dnn.readNet(\"/kaggle/working/yolov3.weights\", \"/kaggle/working/yolov3.cfg\") \n\n# Load COCO class labels\nwith open(\"/kaggle/working/coco.names\", \"r\") as f:\n    classes = [line.strip() for line in f.readlines()]\n\n# Get layer names\nlayer_names = net.getLayerNames()\noutput_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load image\nimage = cv2.imread(\"/kaggle/input/cat-and-dogs/dataset/test_set/cats/cat.4003.jpg\")\nheight, width = image.shape[:2]\n\n# Convert image to blob\nblob = cv2.dnn.blobFromImage(image, scalefactor=1/255.0, size=(416, 416), swapRB=True, crop=False)\nnet.setInput(blob)\n\n# Run YOLO model\nlayer_outputs = net.forward(output_layers)\n\n# Process detections\nboxes, confidences, class_ids = [], [], []\n\nfor output in layer_outputs:\n    for detection in output:\n        scores = detection[5:]\n        class_id = np.argmax(scores)\n        confidence = scores[class_id]\n\n        if confidence > 0.5:\n            center_x, center_y, w, h = (detection[:4] * np.array([width, height, width, height])).astype(\"int\")\n            x, y = int(center_x - w / 2), int(center_y - h / 2)\n            \n            boxes.append([x, y, int(w), int(h)])\n            confidences.append(float(confidence))\n            class_ids.append(class_id)\n\n# Draw bounding boxes without NMS\nfor i in range(len(boxes)):\n    x, y, w, h = boxes[i]\n    label = f\"{classes[class_ids[i]]}: {confidences[i]:.2f}\"\n    color = (0, 255, 0)  # Green box\n    cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n    cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n# Show output image\nimport matplotlib.pyplot as plt\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.axis('off')  # Hide axis\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Applying non maximum suppression","metadata":{}},{"cell_type":"code","source":"# Load image\nimage = cv2.imread(\"/kaggle/input/cat-and-dogs/dataset/test_set/cats/cat.4003.jpg\")\nheight, width = image.shape[:2]\n\n# Convert image to blob\nblob = cv2.dnn.blobFromImage(image, scalefactor=1/255.0, size=(416, 416), swapRB=True, crop=False)\nnet.setInput(blob)\n\n# Run YOLO model\nlayer_outputs = net.forward(output_layers)\n\n# Process detections\nboxes, confidences, class_ids = [], [], []\n\nfor output in layer_outputs:\n    for detection in output:\n        scores = detection[5:]\n        class_id = np.argmax(scores)\n        confidence = scores[class_id]\n\n        if confidence > 0.5:\n            center_x, center_y, w, h = (detection[:4] * np.array([width, height, width, height])).astype(\"int\")\n            x, y = int(center_x - w / 2), int(center_y - h / 2)\n            \n            boxes.append([x, y, int(w), int(h)])\n            confidences.append(float(confidence))\n            class_ids.append(class_id)\n\n# Draw bounding boxes without NMS\n# Apply Non-Maximum Suppression (NMS)\nindices = cv2.dnn.NMSBoxes(boxes, confidences, score_threshold=0.5, nms_threshold=0.4)\n\n# Removes overlapping boxes to keep only the best ones:\n# score_threshold=0.5 → Keep detections with confidence > 50%.\n# nms_threshold=0.4 → If two boxes overlap >40%, keep the one with the highest confidence.\n\n# Draw bounding boxes\nif len(indices) > 0:\n    for i in indices.flatten():\n        x, y, w, h = boxes[i]\n        label = f\"{classes[class_ids[i]]}: {confidences[i]:.2f}\"\n        color = (0, 255, 0)  # Green for bounding boxes\n        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n        cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n# Show the image\nimport matplotlib.pyplot as plt\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.axis('off')  # Hide axis\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Que Why NMS is important ?\n\nAns- Non-Maximum Suppression (NMS) is essential in object detection to eliminate redundant and overlapping bounding boxes. During detection, multiple bounding boxes may be predicted for a single object, often with different confidence scores. NMS helps by keeping the box with the highest confidence score and suppressing others that have a significant overlap (usually defined by the Intersection over Union, IoU threshold). This ensures that each object is detected with only one bounding box, reducing false positives and improving the precision and efficiency of the detection system. Without NMS, multiple detections for the same object can degrade the model’s performance.","metadata":{}},{"cell_type":"markdown","source":"## Que3 Training on Custom dataset","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5.git\n%cd yolov5 \n!pip install -r requirements.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import yaml\nwith open(\"/kaggle/input/yaml-file/data.yaml\", 'r') as f:\n    dataset_yaml = yaml.safe_load(f)\nprint(dataset_yaml)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Train the model\n\n!python train.py --img 640 --batch 8 --epochs 100 --data /kaggle/input/yaml-file/data.yaml --weights yolov5s.pt --cache --device 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Accuracy = 2*p*r/p+r","metadata":{}},{"cell_type":"code","source":"!python val.py --weights runs/train/exp5/weights/best.pt --data /kaggle/input/yaml-file/data.yaml --img 640\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport torch\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\n\n# Run YOLOv5 detection\n!python detect.py --weights runs/train/exp5/weights/best.pt --img 640 --conf 0.5 --source /kaggle/input/yolov5-custom-dataset/test/images/cat_864_jpg.rf.a511d6f078a24ff41971e84a8a5d9922.jpg\n\n# Load and display the output image\nresult_path = \"runs/detect/exp/cat_864_jpg.rf.a511d6f078a24ff41971e84a8a5d9922.jpg\"\n\n# Read and display the image using OpenCV\nimage = cv2.imread(result_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for proper display\n\n# Display using Matplotlib\nplt.figure(figsize=(8, 6))\nplt.imshow(image)\nplt.axis(\"off\")  # Hide axes\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Que 4 On live video feed","metadata":{}},{"cell_type":"code","source":"import cv2\nimport torch\nfrom yolov5 import detect  # Ensure YOLOv5 is in your working directory\n\n# Load YOLOv5 model\nmodel_path = \"runs/train/exp5/weights/best.pt\"  # Path to your trained weights\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Open webcam (0 for default webcam)\ncap = cv2.VideoCapture(0)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break  # Exit if no frame is captured\n\n    # Save the frame temporarily\n    cv2.imwrite(\"temp.jpg\", frame)\n\n    # Run YOLO detection\n    detect.run(weights=model_path, source=\"temp.jpg\", conf_thres=0.5, save_txt=False, save_conf=False)\n\n    # Load the processed image with detections\n    result_img = cv2.imread(\"runs/detect/exp/temp.jpg\")\n    cv2.imshow(\"YOLOv5 Live Detection\", result_img)\n\n    # Press 'q' to exit\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Release resources\ncap.release()\ncv2.destroyAllWindows()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Que 5","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision.transforms import functional as F\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Load Faster R-CNN pre-trained model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nmodel.eval()\n\n# Load COCO class names\nCOCO_INSTANCE_CATEGORY_NAMES = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n    'traffic light', 'fire hydrant', 'street sign', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n    'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'hat', 'backpack', 'umbrella', 'shoe',\n    'eye glasses', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n    'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'plate', 'wine glass', 'cup', 'fork',\n    'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'mirror', 'dining table', 'window', 'desk', 'toilet',\n    'door', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n    'refrigerator', 'blender', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\n# Load and preprocess image\nimage_path = \"/kaggle/input/yolov5-custom-dataset/test/images/cat_910_jpg.rf.c61d1b561b0da07cf271edecd4b6f6d5.jpg\"\nimage = Image.open(image_path).convert(\"RGB\")\nimage_tensor = F.to_tensor(image).unsqueeze(0)  # Convert to tensor and add batch dimension\n\n# Perform inference\nwith torch.no_grad():\n    predictions = model(image_tensor)\n\n# Extract detections\nboxes = predictions[0]['boxes']\nlabels = predictions[0]['labels']\nscores = predictions[0]['scores']\n\n# Convert image for OpenCV\nimage_cv = cv2.imread(image_path)\nimage_cv = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)\n\n# Draw detections on image\nfor i in range(len(boxes)):\n    if scores[i] > 0.5:  # Confidence threshold\n        box = boxes[i].numpy().astype(\"int\")\n        label = COCO_INSTANCE_CATEGORY_NAMES[labels[i]]  # Convert label index to name\n        confidence = scores[i].item()\n\n        # Draw bounding box\n        cv2.rectangle(image_cv, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n        cv2.putText(image_cv, f\"{label}: {confidence:.2f}\", (box[0], box[1] - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n\n# Show the image with detections\nplt.figure(figsize=(8, 6))\nplt.imshow(image_cv)\nplt.axis(\"off\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Que 6","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision pycocotools","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.datasets import CocoDetection\nfrom torchvision import transforms\nimport os\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading the dataset\n\nclass CustomCocoDataset(CocoDetection):\n    def __init__(self, img_folder, ann_file, transforms=None):\n        super().__init__(img_folder, ann_file)\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n        img, target = super().__getitem__(idx)\n        if self.transforms:\n            img = self.transforms(img)\n        return img, target\n\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\n\ntrain_dataset = CustomCocoDataset(\n    img_folder='/kaggle/input/catanddog-dataset-coco-format/train/images',\n    ann_file='/kaggle/input/catanddog-dataset-coco-format/train/_annotations.coco.json',\n    transforms=transform\n)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load pre-trained Faster R-CNN model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# Modify the classifier for two classes (Background + 2 classes: Cat & Dog)\nnum_classes = 3  # Background + Cat + Dog\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training parameters\nnum_epochs = 10\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for images, targets in train_loader:\n        images = [img.to(device) for img in images]\n        targets = [{k: v for k, v in t.items()} for t in targets]  # Ensure targets are correctly formatted\n\n        loss_dict = model(images, targets)\n        loss = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n\nprint(\"Training complete! ✅\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Faster R-CNN requires images along with bounding box annotations in Pascal VOC (XML) or COCO (JSON) format. Since I am using YOLO format (TXT), I need to convert it to the required format.","metadata":{}},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\n\ndef yolo_to_voc(yolo_label, img_width, img_height):\n    class_id, x_center, y_center, width, height = map(float, yolo_label)\n    x_center *= img_width\n    y_center *= img_height\n    width *= img_width\n    height *= img_height\n    \n    xmin = int(x_center - width / 2)\n    ymin = int(y_center - height / 2)\n    xmax = int(x_center + width / 2)\n    ymax = int(y_center + height / 2)\n    \n    return xmin, ymin, xmax, ymax\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, let’s define a Custom Dataset Class for Faster R-CNN:","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, image_dir, label_dir, transforms=None):\n        self.image_dir = image_dir\n        self.label_dir = label_dir\n        self.transforms = transforms\n        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n        self.label_files = [f.replace('.jpg', '.txt') for f in self.image_files]\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        # Load image\n        img_path = os.path.join(self.image_dir, self.image_files[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        img_width, img_height = img.size\n\n        # Load annotations\n        label_path = os.path.join(self.label_dir, self.label_files[idx])\n        boxes = []\n        labels = []\n\n        with open(label_path, \"r\") as f:\n            for line in f:\n                label_data = line.strip().split()\n                class_id = int(label_data[0]) + 1  # Pascal VOC index starts from 1\n                xmin, ymin, xmax, ymax = yolo_to_voc(label_data[1:], img_width, img_height)\n                \n                boxes.append([xmin, ymin, xmax, ymax])\n                labels.append(class_id)\n\n        boxes = torch.tensor(boxes, dtype=torch.float32)\n        labels = torch.tensor(labels, dtype=torch.int64)\n        target = {\"boxes\": boxes, \"labels\": labels}\n\n        if self.transforms:\n            img = self.transforms(img)\n\n        return img, target\n\n# Apply image transformations\ntransform = transforms.Compose([transforms.ToTensor()])\n\n# Load datasets\ntrain_dataset = CustomDataset(\"/kaggle/input/yolov5-custom-dataset/train/images\", \"/kaggle/input/yolov5-custom-dataset/train/labels\", transforms=transform)\nval_dataset = CustomDataset(\"/kaggle/input/yolov5-custom-dataset/valid/images\", \"/kaggle/input/yolov5-custom-dataset/valid/labels\", transforms=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\nval_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading faster rcnn model \n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# Load pre-trained Faster R-CNN model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# Modify classifier to match the number of classes in our dataset\nnum_classes = 3  # (background, cat, dog)\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training\n\nimport torch.optim as optim\n\n# Set optimizer and learning rate\noptimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n# Define the training function\ndef train_model(model, train_loader, val_loader, num_epochs=10):\n    model.train()\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        for images, targets in train_loader:\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            loss_dict = model(images, targets)\n            loss = sum(loss for loss in loss_dict.values())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n\ntrain_model(model, train_loader, val_loader, num_epochs=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def __getitem__(self, idx):\n    ...\n    with open(label_path, \"r\") as f:\n        for line in f:\n            label_data = line.strip().split()\n            print(f\"DEBUG: Label data -> {label_data}\")  # Debug print\n\n            if len(label_data) != 5:\n                print(f\"ERROR: Invalid label format in {label_path}\")\n                continue  # Skip the faulty label\n            \n            class_id = int(label_data[0]) + 1\n            xmin, ymin, xmax, ymax = yolo_to_voc(label_data[1:], img_width, img_height)\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(class_id)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## End! Thank You....","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}