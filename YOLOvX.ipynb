{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For yolov3 you can prefer: 1- https://pjreddie.com/media/files/papers/YOLOv3.pdf and 2- https://pjreddie.com/darknet/yolo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Que 1\n",
    "\n",
    "!git clone https://github.com/pjreddie/darknet\n",
    "%cd darknet\n",
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wget https://pjreddie.com/media/files/yolov3.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!./darknet detect cfg/yolov3.cfg yolov3.weights /kaggle/input/cat-and-dogs/dataset/test_set/cats/cat.4004.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the above with open-cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wget https://pjreddie.com/media/files/yolov3.weights \n",
    "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg \n",
    "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"/kaggle/working/yolov3.weights\", \"/kaggle/working/yolov3.cfg\") \n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"/kaggle/working/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Get layer names\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "image = cv2.imread(\"/kaggle/input/cat-and-dogs/dataset/test_set/cats/cat.4003.jpg\")\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# Convert image to blob\n",
    "blob = cv2.dnn.blobFromImage(image, scalefactor=1/255.0, size=(416, 416), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "\n",
    "# Run YOLO model\n",
    "layer_outputs = net.forward(output_layers)\n",
    "\n",
    "# Process detections\n",
    "boxes, confidences, class_ids = [], [], []\n",
    "\n",
    "for output in layer_outputs:\n",
    "    for detection in output:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "\n",
    "        if confidence > 0.5:\n",
    "            center_x, center_y, w, h = (detection[:4] * np.array([width, height, width, height])).astype(\"int\")\n",
    "            x, y = int(center_x - w / 2), int(center_y - h / 2)\n",
    "            \n",
    "            boxes.append([x, y, int(w), int(h)])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n",
    "\n",
    "# Draw bounding boxes without NMS\n",
    "for i in range(len(boxes)):\n",
    "    x, y, w, h = boxes[i]\n",
    "    label = f\"{classes[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "    color = (0, 255, 0)  # Green box\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "    cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "# Show output image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying non maximum suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "image = cv2.imread(\"/kaggle/input/cat-and-dogs/dataset/test_set/cats/cat.4003.jpg\")\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# Convert image to blob\n",
    "blob = cv2.dnn.blobFromImage(image, scalefactor=1/255.0, size=(416, 416), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "\n",
    "# Run YOLO model\n",
    "layer_outputs = net.forward(output_layers)\n",
    "\n",
    "# Process detections\n",
    "boxes, confidences, class_ids = [], [], []\n",
    "\n",
    "for output in layer_outputs:\n",
    "    for detection in output:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "\n",
    "        if confidence > 0.5:\n",
    "            center_x, center_y, w, h = (detection[:4] * np.array([width, height, width, height])).astype(\"int\")\n",
    "            x, y = int(center_x - w / 2), int(center_y - h / 2)\n",
    "            \n",
    "            boxes.append([x, y, int(w), int(h)])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n",
    "\n",
    "# Draw bounding boxes without NMS\n",
    "# Apply Non-Maximum Suppression (NMS)\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, score_threshold=0.5, nms_threshold=0.4)\n",
    "\n",
    "# Removes overlapping boxes to keep only the best ones:\n",
    "# score_threshold=0.5 → Keep detections with confidence > 50%.\n",
    "# nms_threshold=0.4 → If two boxes overlap >40%, keep the one with the highest confidence.\n",
    "\n",
    "# Draw bounding boxes\n",
    "if len(indices) > 0:\n",
    "    for i in indices.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = f\"{classes[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "        color = (0, 255, 0)  # Green for bounding boxes\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "# Show the image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que Why NMS is important ?\n",
    "\n",
    "Ans- Non-Maximum Suppression (NMS) is essential in object detection to eliminate redundant and overlapping bounding boxes. During detection, multiple bounding boxes may be predicted for a single object, often with different confidence scores. NMS helps by keeping the box with the highest confidence score and suppressing others that have a significant overlap (usually defined by the Intersection over Union, IoU threshold). This ensures that each object is detected with only one bounding box, reducing false positives and improving the precision and efficiency of the detection system. Without NMS, multiple detections for the same object can degrade the model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que3 Training on Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5.git\n",
    "%cd yolov5 \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"/kaggle/input/yaml-file/data.yaml\", 'r') as f:\n",
    "    dataset_yaml = yaml.safe_load(f)\n",
    "print(dataset_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Train the model\n",
    "\n",
    "!python train.py --img 640 --batch 8 --epochs 100 --data /kaggle/input/yaml-file/data.yaml --weights yolov5s.pt --cache --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = 2*p*r/p+r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python val.py --weights runs/train/exp5/weights/best.pt --data /kaggle/input/yaml-file/data.yaml --img 640\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Run YOLOv5 detection\n",
    "!python detect.py --weights runs/train/exp5/weights/best.pt --img 640 --conf 0.5 --source /kaggle/input/yolov5-custom-dataset/test/images/cat_864_jpg.rf.a511d6f078a24ff41971e84a8a5d9922.jpg\n",
    "\n",
    "# Load and display the output image\n",
    "result_path = \"runs/detect/exp/cat_864_jpg.rf.a511d6f078a24ff41971e84a8a5d9922.jpg\"\n",
    "\n",
    "# Read and display the image using OpenCV\n",
    "image = cv2.imread(result_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for proper display\n",
    "\n",
    "# Display using Matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que 4 On live video feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from yolov5 import detect  # Ensure YOLOv5 is in your working directory\n",
    "\n",
    "# Load YOLOv5 model\n",
    "model_path = \"runs/train/exp5/weights/best.pt\"  # Path to your trained weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Open webcam (0 for default webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Exit if no frame is captured\n",
    "\n",
    "    # Save the frame temporarily\n",
    "    cv2.imwrite(\"temp.jpg\", frame)\n",
    "\n",
    "    # Run YOLO detection\n",
    "    detect.run(weights=model_path, source=\"temp.jpg\", conf_thres=0.5, save_txt=False, save_conf=False)\n",
    "\n",
    "    # Load the processed image with detections\n",
    "    result_img = cv2.imread(\"runs/detect/exp/temp.jpg\")\n",
    "    cv2.imshow(\"YOLOv5 Live Detection\", result_img)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load Faster R-CNN pre-trained model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load COCO class names\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "    'traffic light', 'fire hydrant', 'street sign', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
    "    'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'hat', 'backpack', 'umbrella', 'shoe',\n",
    "    'eye glasses', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n",
    "    'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'plate', 'wine glass', 'cup', 'fork',\n",
    "    'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'mirror', 'dining table', 'window', 'desk', 'toilet',\n",
    "    'door', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "    'refrigerator', 'blender', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# Load and preprocess image\n",
    "image_path = \"/kaggle/input/yolov5-custom-dataset/test/images/cat_910_jpg.rf.c61d1b561b0da07cf271edecd4b6f6d5.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_tensor = F.to_tensor(image).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(image_tensor)\n",
    "\n",
    "# Extract detections\n",
    "boxes = predictions[0]['boxes']\n",
    "labels = predictions[0]['labels']\n",
    "scores = predictions[0]['scores']\n",
    "\n",
    "# Convert image for OpenCV\n",
    "image_cv = cv2.imread(image_path)\n",
    "image_cv = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Draw detections on image\n",
    "for i in range(len(boxes)):\n",
    "    if scores[i] > 0.5:  # Confidence threshold\n",
    "        box = boxes[i].numpy().astype(\"int\")\n",
    "        label = COCO_INSTANCE_CATEGORY_NAMES[labels[i]]  # Convert label index to name\n",
    "        confidence = scores[i].item()\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image_cv, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
    "        cv2.putText(image_cv, f\"{label}: {confidence:.2f}\", (box[0], box[1] - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "# Show the image with detections\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(image_cv)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Que 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "class CustomCocoDataset(CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms=None):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super().__getitem__(idx)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, target\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CustomCocoDataset(\n",
    "    img_folder='/kaggle/input/catanddog-dataset-coco-format/train/images',\n",
    "    ann_file='/kaggle/input/catanddog-dataset-coco-format/train/_annotations.coco.json',\n",
    "    transforms=transform\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Modify the classifier for two classes (Background + 2 classes: Cat & Dog)\n",
    "num_classes = 3  # Background + Cat + Dog\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v for k, v in t.items()} for t in targets]  # Ensure targets are correctly formatted\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete! ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End! Thank You...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1394179,
     "sourceId": 2310932,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7030981,
     "sourceId": 11251419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7031089,
     "sourceId": 11251558,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7031988,
     "sourceId": 11252787,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
